{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CykWBv31VsGS"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VuAA5mfX5c2B",
        "outputId": "05bd257c-c767-436d-bb6a-f655b9538fdd"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install lightning\n",
        "!pip install wandb\n",
        "!pip install gdown\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lddwxqafLvaV"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BsDt96LFT53"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn  \n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "from torchmetrics.classification import MulticlassRecall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwWWqLN05Lho"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4fg6dL-xKVCK"
      },
      "outputs": [],
      "source": [
        "!gdown 15hLDYO17KUvP9O0TMVx-7U7Muqm4q5PS\n",
        "!gdown 1PmGhoMOXTLvceGCo-btUpPl8_MtIKWzR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyr6D9b4LFK2"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-kE_a1ULH6X"
      },
      "outputs": [],
      "source": [
        "class TripletDataset(Dataset):\n",
        "  def __init__(self, model):\n",
        "    # read csv_file into df \n",
        "    self.duplicates_df = pd.read_csv(\"pandas_duplicates.csv\") \n",
        "    self.negatives_df = pd.read_csv(\"pandas_negatives.csv\")\n",
        "\n",
        "    # tokenizer\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    \n",
        "    self.max_seq_len = 100  \n",
        "  \n",
        "  def __len__(self):\n",
        "    return 100\n",
        "    # return min(len(self.duplicates_df.index), len(self.negatives_df.index))\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    # read df \n",
        "    dup1_text = str(self.duplicates_df.at[idx, \"NaturalLang1\"])\n",
        "    dup1_code = str(self.duplicates_df.at[idx, \"ProgrammingLang1\"])\n",
        "    dup2_text = str(self.duplicates_df.at[idx, \"NaturalLang2\"])\n",
        "    dup2_code = str(self.duplicates_df.at[idx, \"ProgrammingLang2\"])\n",
        "\n",
        "    neg_text = str(self.negatives_df.at[idx, \"NaturalLang\"])\n",
        "    neg_code = str(self.negatives_df.at[idx, \"ProgrammingLang\"])\n",
        "\n",
        "\n",
        "    # tokenize\n",
        "    dup1 = self.tokenizer(dup1_text, dup1_code, truncation=True, max_length=self.max_seq_len, return_tensors=\"pt\")\n",
        "    dup2 = self.tokenizer(dup2_text, dup2_code, truncation=True, max_length=self.max_seq_len, return_tensors=\"pt\")\n",
        "    neg = self.tokenizer(neg_text, neg_code, truncation=True, max_length=self.max_seq_len, return_tensors=\"pt\")\n",
        "\n",
        "    # squeeze dimensions (necessary for batching)\n",
        "    dup1['input_ids'] = torch.squeeze(dup1['input_ids'])\n",
        "    dup1['attention_mask'] = torch.squeeze(dup1['attention_mask'])\n",
        "    dup2['input_ids'] = torch.squeeze(dup2['input_ids'])\n",
        "    dup2['attention_mask'] = torch.squeeze(dup2['attention_mask'])\n",
        "    neg['input_ids'] = torch.squeeze(neg['input_ids'])\n",
        "    neg['attention_mask'] = torch.squeeze(neg['attention_mask'])\n",
        "\n",
        "    return dup1, dup2, neg "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnAMkQnvNGI9"
      },
      "source": [
        "# dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTdi4KQWNHSK"
      },
      "outputs": [],
      "source": [
        "# dynamic padding -> pad all sequences to longest sequence in current batch \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"UWB-AIR/MQDD-pretrained\")\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "  dups1 = [item[0] for item in batch] # dup1\n",
        "  dups2 = [item[1] for item in batch] # dup2\n",
        "  negs = [item[2] for item in batch] # neg\n",
        "\n",
        "  dups1 = data_collator(dups1)\n",
        "  dups2 = data_collator(dups2)\n",
        "  negs = data_collator(negs)\n",
        "\n",
        "  return dups1, dups2, negs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqwzANhzVQSz"
      },
      "source": [
        "# triplet loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID_s4OEOeUEb"
      },
      "outputs": [],
      "source": [
        "def get_triplet_loss_labels(batch_size):\n",
        "  \"\"\"\n",
        "  N = batch size \n",
        "  labels = [0, ..., N-1, 0, ..., N-1, N, ..., 2N-1]\n",
        "  \"\"\"\n",
        "  d_labels = torch.arange(batch_size) \n",
        "  n_labels = torch.arange(batch_size, 2*batch_size)\n",
        "  labels = torch.cat((d_labels, d_labels, n_labels)) \n",
        "  labels = labels.to(device)\n",
        "\n",
        "  return labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R_Cs8s0VSad"
      },
      "outputs": [],
      "source": [
        "def calc_pairwise_distances(embeddings):\n",
        "  \"\"\"\n",
        "  distances: (B, B)\n",
        "  distances[i, j] = squared euclidian distance of embeddings i and j\n",
        "  \"\"\"\n",
        "  dot_product = embeddings @ embeddings.T \n",
        "  square_norm = torch.diag(dot_product) \n",
        "  distances = square_norm.unsqueeze(0) - 2*dot_product + square_norm.unsqueeze(1)\n",
        "  distances[distances < 0] = 0 \n",
        "  return distances\n",
        "\n",
        "def get_triplet_mask(labels):\n",
        "  \"\"\"\n",
        "  mask[i, j, k] = True if and only if:\n",
        "    - i != j != k  \n",
        "    - labels[i] == labels[j] and labels[i] != labels[k]\n",
        "  \"\"\"\n",
        "  n = labels.shape[0]\n",
        "  device = labels.device\n",
        "\n",
        "  # indices\n",
        "  indices_neq = ~torch.eye(n, device=device).bool() \n",
        "\n",
        "  i_neq_j = indices_neq.unsqueeze(2)\n",
        "  i_neq_k = indices_neq.unsqueeze(1)\n",
        "  j_neq_k = indices_neq.unsqueeze(0) \n",
        "\n",
        "  distinct_indices = i_neq_j & i_neq_k & j_neq_k \n",
        "\n",
        "  # labels \n",
        "  labels_eq = (labels.unsqueeze(0) == labels.unsqueeze(1)) \n",
        "\n",
        "  i_eq_j = labels_eq.unsqueeze(2)\n",
        "  i_eq_k = labels_eq.unsqueeze(1) \n",
        "\n",
        "  valid_labels = i_eq_j & (~i_eq_k)\n",
        "\n",
        "  valid_triplets = distinct_indices & valid_labels \n",
        "  return valid_triplets\n",
        "\n",
        "def get_anchor_positive_triplet_mask(labels):\n",
        "  \"\"\"\n",
        "  mask[i, j] True iff:\n",
        "    - i != j\n",
        "    - labels[i] == labels[j]\n",
        "  \"\"\"\n",
        "  n = labels.shape[0]\n",
        "  device = labels.device\n",
        "\n",
        "  # indices\n",
        "  indices_neq = ~torch.eye(n, device=device).bool() \n",
        "\n",
        "  # labels\n",
        "  labels_eq = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
        "\n",
        "  return labels_eq & indices_neq\n",
        "\n",
        "def get_anchor_negative_triplet_mask(labels):\n",
        "  \"\"\"\n",
        "  mask[i, k] True iff\n",
        "    - labels[i] ~= labels[k]\n",
        "  \"\"\"\n",
        "  return ~(labels.unsqueeze(0) == labels.unsqueeze(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uRU28-iVd7q"
      },
      "outputs": [],
      "source": [
        "def batch_all_triplet_loss(embeddings, labels, margin=0.05):\n",
        "  pairwise_distances = calc_pairwise_distances(embeddings)\n",
        "\n",
        "  anchor_positive_distances = pairwise_distances.unsqueeze(2) # (4,4,1)\n",
        "  anchor_negative_distances = pairwise_distances.unsqueeze(1) # (4,1,4)\n",
        "\n",
        "  triplet_loss = anchor_positive_distances - anchor_negative_distances + margin \n",
        "\n",
        "  mask = get_triplet_mask(labels)\n",
        "  triplet_loss = triplet_loss * mask.float().to(device) # valid triplets\n",
        "\n",
        "  triplet_loss = F.relu(triplet_loss) # semi-hard/hard triplets \n",
        "\n",
        "  positive_triplets = triplet_loss[triplet_loss > 1e-16]\n",
        "  num_positive_triplets = positive_triplets.size(0)\n",
        "\n",
        "  triplet_loss = triplet_loss.sum() / (num_positive_triplets + 1e-16)\n",
        "\n",
        "  return triplet_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnjYtUwMVflc"
      },
      "outputs": [],
      "source": [
        "def batch_hard_triplet_loss(embeddings, labels, margin=0.05):\n",
        "  pairwise_distances = calc_pairwise_distances(embeddings)\n",
        "\n",
        "  # hardest positive\n",
        "  anchor_positive_mask = get_anchor_positive_triplet_mask(labels).float()\n",
        "  anchor_positive_distances = pairwise_distances * anchor_positive_mask \n",
        "  hardest_positive_distances, _ = anchor_positive_distances.max(1, keepdim=True)\n",
        "\n",
        "  # hardest negative\n",
        "  anchor_negative_mask = get_anchor_negative_triplet_mask(labels).float()\n",
        "  max_anchor_negative_distances, _ = pairwise_distances.max(1, keepdim=True)\n",
        "  anchor_negative_distances = pairwise_distances + max_anchor_negative_distances * (1.0 - anchor_negative_mask)\n",
        "  hardest_negative_distances, _ = anchor_negative_distances.min(1, keepdim=True) \n",
        "\n",
        "  # loss\n",
        "  triplet_loss = hardest_positive_distances - hardest_negative_distances + margin \n",
        "  triplet_loss = F.relu(triplet_loss)\n",
        "  triplet_loss = triplet_loss.mean()\n",
        "  return triplet_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FirK_-B_cg0S"
      },
      "source": [
        "# evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bfKIwGEciZ4"
      },
      "outputs": [],
      "source": [
        "def calc_pairwise_cosine_similarity(embeddings):\n",
        "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    pairwise_cos_sims = embeddings @ embeddings.T\n",
        "    return pairwise_cos_sims.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MspcEhDCeH0"
      },
      "outputs": [],
      "source": [
        "def get_recall_labels(batch_size):\n",
        "  l1 = torch.arange(batch_size, 2*batch_size)\n",
        "  l2 = torch.arange(batch_size)\n",
        "  labels = torch.cat((l1, l2)).long()\n",
        "  return labels.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmeqDjIlILzk"
      },
      "source": [
        "# pytorch lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpTccRYwXcJd"
      },
      "outputs": [],
      "source": [
        "class TripletLightningModule(pl.LightningModule):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.model_name = config['model']\n",
        "    self.model = AutoModel.from_pretrained(config['model'])\n",
        "    \n",
        "    self.batch_size = config['batch_size']\n",
        "\n",
        "    self.loss = config['loss'] \n",
        "    self.margin = config['margin']\n",
        "\n",
        "    self.lr = config['lr']\n",
        "\n",
        "    self.save_hyperparameters() \n",
        "\n",
        "    # triplet loss\n",
        "    self.triplet_labels = get_triplet_loss_labels(config['batch_size']).to(device)\n",
        "\n",
        "    # recall \n",
        "    self.recall_labels = get_recall_labels(self.batch_size).to(device)\n",
        "\n",
        "    self.train_recall = MulticlassRecall(num_classes=config['batch_size']*3, top_k=5).to(device)\n",
        "    self.val_recall = MulticlassRecall(num_classes=config['batch_size']*3, top_k=5).to(device)\n",
        "    self.test_recall = MulticlassRecall(num_classes=config['batch_size']*3, top_k=5).to(device)\n",
        "\n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    embeddings = self._get_embeddings(batch)\n",
        "    \n",
        "    # loss \n",
        "    loss = self._get_loss(embeddings)\n",
        "\n",
        "    # recall \n",
        "    dupl_cos_sims = self._get_dupl_cos_sims(embeddings).to(device)\n",
        "    recall = self.train_recall(dupl_cos_sims, self.recall_labels)\n",
        "  \n",
        "    self.log(\"train_loss\", loss, on_step=True)\n",
        "    self.log(\"train_recall\", recall, on_step=True)\n",
        "\n",
        "    return loss \n",
        "  \n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    embeddings = self._get_embeddings(batch)\n",
        "    \n",
        "    # loss\n",
        "    loss = self._get_loss(embeddings)\n",
        "    \n",
        "    # recall\n",
        "    dupl_cos_sims = self._get_dupl_cos_sims(embeddings).to(device)\n",
        "    recall = self.val_recall(dupl_cos_sims, self.recall_labels)\n",
        "    \n",
        "    # log\n",
        "    self.log(\"val_loss\", loss)\n",
        "    self.log(\"val_recall\", recall)\n",
        "\n",
        "    return loss \n",
        "  \n",
        "  def test_step(self, batch, batch_idx):\n",
        "    embeddings = self._get_embeddings(batch)\n",
        "    \n",
        "    # loss\n",
        "    loss = self._get_loss(embeddings)\n",
        "    \n",
        "    # recall\n",
        "    dupl_cos_sims = self._get_dupl_cos_sims(embeddings).to(device)\n",
        "    recall = self.test_recall(dupl_cos_sims, self.recall_labels)\n",
        "    \n",
        "    # log\n",
        "    self.log(\"val_loss\", loss)\n",
        "    self.log(\"val_recall\", recall)\n",
        "\n",
        "    return loss \n",
        "\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "    return optimizer \n",
        "\n",
        "\n",
        "  def _get_embeddings(self, batch):\n",
        "    d1, d2, n = batch \n",
        "\n",
        "    d1 = self.model(**d1)[1] \n",
        "    d2 = self.model(**d2)[1] \n",
        "    n = self.model(**n)[1] \n",
        "\n",
        "    embeddings = torch.cat((d1, d2, n), dim=0) \n",
        "    return embeddings\n",
        "  \n",
        "  def _get_loss(self, embeddings):\n",
        "    if self.loss == 'all':\n",
        "      loss = batch_all_triplet_loss(embeddings, self.triplet_labels, margin=self.margin)\n",
        "    else:\n",
        "      loss = batch_hard_triplet_loss(embeddings, self.triplet_labels, margin=self.margin)\n",
        "    \n",
        "    return loss \n",
        "\n",
        "  def _get_dupl_cos_sims(self, embeddings):\n",
        "    pairwise_cos_sims = calc_pairwise_cosine_similarity(embeddings)\n",
        "    \n",
        "    # mask out diagonal\n",
        "    mask = 1 - torch.eye(self.batch_size*3).float().to(device)\n",
        "    pairwise_cos_sims *= mask\n",
        "    \n",
        "    # ignore negatives \n",
        "    dupl_cos_sims = pairwise_cos_sims[:2*self.batch_size]\n",
        "    return dupl_cos_sims.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKe6ZTExXaMR"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIgIrHv-I7vQ"
      },
      "outputs": [],
      "source": [
        "losses = ['all', 'hard']\n",
        "models = ['UWB-AIR/MQDD-pretrained', 'microsoft/codebert-base']\n",
        "\n",
        "config = {\n",
        "    'model': models[0]\n",
        "    'batch_size': 4,\n",
        "    'loss': losses[0],\n",
        "    'margin': 0.05,\n",
        "    'lr': 0.001,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU68aKTzMrZG"
      },
      "outputs": [],
      "source": [
        "dataset = TripletDataset(config['model'])\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.15, 0.15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GrGvpaoNNqQ"
      },
      "outputs": [],
      "source": [
        "batch_size = config['batch_size'] \n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, drop_last=True, collate_fn=my_collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size, drop_last=True, collate_fn=my_collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size, drop_last=True, collate_fn=my_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9lK07qDGYT2v"
      },
      "outputs": [],
      "source": [
        "project_name = 'stack-overflow-duplicate-detection'  \n",
        "\n",
        "wandb.init(project=project_name)\n",
        "wandb_logger = WandbLogger(project=project_name, log_model='all') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT61DYskYZ-q"
      },
      "outputs": [],
      "source": [
        "triplet_lightning_module = TripletLightningModule(config)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min')\n",
        "trainer = pl.Trainer(max_epochs=5, logger=wandb_logger, callbacks=[checkpoint_callback], log_every_n_steps=1)\n",
        "trainer.fit(triplet_lightning_module, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzV27_NFyvmq"
      },
      "outputs": [],
      "source": [
        "trainer.test(triplet_lightning_module, dataloaders=test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CykWBv31VsGS",
        "Fyr6D9b4LFK2",
        "nnAMkQnvNGI9",
        "wqwzANhzVQSz",
        "FirK_-B_cg0S"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
